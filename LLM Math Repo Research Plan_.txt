Resources for Large Language Models in Mathematics: A Curated Overview
1. Introduction
Purpose: The field of Artificial Intelligence (AI) has witnessed transformative progress with the advent of Large Language Models (LLMs). These models, trained on vast datasets, exhibit remarkable capabilities in understanding and generating human language. Recently, significant research interest has focused on leveraging LLMs for complex reasoning tasks, with mathematics serving as a critical domain for evaluating and pushing the boundaries of AI cognition.1 This report aims to provide a comprehensive research foundation for the development of a curated repository, tentatively named "awesome-LLM-in-math," dedicated to organizing recent and relevant resources concerning the application of LLMs to mathematical reasoning.
Importance: Mathematical reasoning represents a fundamental aspect of human intelligence, essential across education, science, and engineering.1 Consequently, the ability of AI systems to perform mathematical tasks, ranging from solving grade-school word problems to assisting with university-level proofs and even formal theorem proving, is considered a key indicator of progress towards more general artificial intelligence.1 The evolution of LLMs in this area is marked by a notable shift from foundational models, often characterized by fast, intuitive (System 1-like) responses that may lack depth for complex logic, towards more sophisticated "reasoning LLMs." These newer models aim to emulate the slower, deliberate, step-by-step analysis (System 2-like thinking) required for intricate mathematical problem-solving and proof generation.3 This advancement underscores the growing need for structured resources to track developments in this dynamic field.
Scope Overview: This report synthesizes research findings from the last 2-3 years. It begins by examining the existing landscape of curated resource lists and survey papers relevant to LLMs and reasoning. It then delves into core research areas, including mathematical problem solving, automated theorem proving, symbolic mathematics, and the emerging field of multimodal mathematical reasoning, detailing the key techniques employed in each. Subsequently, it highlights prominent LLMs, both general-purpose and those specialized for mathematics, along with essential datasets and benchmarks used for training and evaluation. Based on an analysis of existing curation practices, a logical structure for the "awesome-LLM-in-math" repository is proposed. The report includes previews of curated lists of papers, code repositories, and datasets organized according to this structure. Finally, it concludes by summarizing the persistent challenges and outlining promising future directions in the application of LLMs to mathematics.
2. Landscape of Curated Resources & Surveys
The rapid expansion of research at the intersection of LLMs and mathematical reasoning has spurred numerous efforts to organize and synthesize information. Understanding this existing landscape is crucial for developing a valuable and non-redundant curated repository.
Existing "Awesome" Lists:
A search reveals a multitude of GitHub repositories adopting the "awesome list" format to curate resources related to LLMs, often touching upon reasoning capabilities. Notable examples include:
* Reasoning-Focused Lists: Repositories like luban-agi/Awesome-LLM-reasoning 9, atfortes/Awesome-LLM-Reasoning 10, and zzli2022/Awesome-System2-Reasoning-LLM 6 specifically target LLM reasoning. They often categorize papers and resources by technique, such as Chain-of-Thought (CoT), Graph-of-Thought (GoT), Backward Reasoning, Tree Search (MCTS), Reinforcement Learning (RL), and Self-Improvement/Self-Training.6 These lists frequently highlight recent advancements and specific model families or techniques like OpenAI's o1/o3 or DeepSeek's R1 models, often associated with "System 2" reasoning.6
* Multimodal Lists: Repositories such as InfiMM/Awesome-Multimodal-LLM-for-Math-STEM 11 and turna1/Awesome-Multmodal_LLM 12 focus on multimodal LLMs (MLLMs), including their application to STEM fields and mathematics. These lists typically organize resources by modality (image, text, etc.) or task, and are valuable for identifying multimodal datasets and benchmarks relevant to math.11
* General & Specialized LLM Lists: Broader lists like Hannibal046/Awesome-LLM 13, jd-coderepos/awesome-llms 14, horseee/Awesome-Efficient-LLM 15, and JShollaj/awesome-llm-interpretability 16 cover various aspects of LLMs, including trending projects, open models, deployment tools, efficiency techniques, and interpretability research. While not exclusively focused on math, they often contain relevant models, benchmarks (like MathEval 13), and techniques applicable to mathematical reasoning.
Common organizational structures observed in these lists involve categorization by resource type (papers, code, datasets), specific techniques (CoT, RL, Quantization), tasks (reasoning, generation, agent learning), or model families.6 The sheer number and varied focus of these existing lists underscore the vibrancy of LLM research but also its fragmentation. While many resources exist, they are often siloed within specific sub-topics (general reasoning, multimodality, efficiency, interpretability). This observation supports the need for a dedicated "awesome-LLM-in-math" repository that integrates these different facets specifically through the lens of mathematical applications.
Key Survey Papers:
Complementing these curated lists are numerous recent academic surveys attempting to structure the field:
* Math-Specific Surveys: Several surveys directly address LLMs in mathematical reasoning and optimization. For instance, arXiv:2503.17726 provides a broad overview covering evolution from PLMs to LLMs, methodologies like CoT, instruction tuning, tool use, RL, and hybrid approaches, alongside models, datasets, challenges, and future directions in math reasoning and optimization.1 arXiv:2502.14333 specifically focuses on feedback-based multi-step reasoning strategies (step-level vs. outcome-level, training-based vs. training-free) for math problems.18
* Reasoning & System 2 Surveys: arXiv:2502.17419 delves into the concept of "Reasoning LLMs" mimicking System 2 thinking, discussing core methods like structure search (MCTS), reward modeling (PRM/ORM), self-improvement, macro actions, and reinforcement fine-tuning.3 Other surveys cover general reasoning mechanisms 17 or specific techniques like CoT.17
* Multimodal Math Surveys: arXiv:2412.11936 offers the first comprehensive survey specifically on multimodal mathematical reasoning, categorizing benchmarks, methodologies (LLM as Reasoner/Enhancer/Planner), and challenges in integrating visual and textual information for math tasks.2
* General LLM Surveys: Broader surveys like arXiv:2402.06196 provide context by reviewing prominent LLM families (GPT, LLaMA, PaLM), training techniques, datasets, and evaluation metrics across various NLP tasks.22 Surveys on LLM evaluation 23 are also relevant for understanding benchmarking practices.
These surveys consistently chart a progression within the field. Early successes often relied on basic prompting techniques, but researchers quickly moved towards more sophisticated strategies to tackle complex reasoning. The introduction of "thought" processes, represented as intermediate tokens 24, led to methods like Chain-of-Thought (CoT).17 Subsequent work built upon this, exploring more complex reasoning structures like Tree-of-Thoughts (ToT) and Graph-of-Thoughts (GoT), integrating external tools (Tool Use), employing Reinforcement Learning (RL) with process or outcome rewards, and developing self-improvement loops where models learn from their own generated reasoning.1 Concurrently, these surveys highlight persistent challenges, including the tendency for LLMs to "hallucinate" incorrect steps, maintain logical consistency, achieve numerical precision, and the difficulty in creating robust evaluation metrics.1 This documented evolution and the identified challenges provide a clear map of the key topics and techniques that an "awesome-LLM-in-math" repository must comprehensively cover to be truly useful.
3. Core Research Areas & Techniques in LLM Mathematical Reasoning
The application of LLMs to mathematics spans several distinct but interconnected research areas, each employing a growing repertoire of specialized techniques.
3.1 Mathematical Problem Solving
This area focuses on enabling LLMs to solve mathematical problems, typically presented in natural language (e.g., word problems) or standard mathematical notation. The complexity ranges from grade-school arithmetic to challenging competition-level problems found in Olympiads or exams like the AIME.26
* Key Techniques:
   * Chain-of-Thought (CoT) and Variants: The foundational technique involves prompting the LLM to output intermediate reasoning steps before the final answer.17 This has evolved into advanced forms involving self-checking, reflection, planning 17, generating longer reasoning chains for complex problems (Long CoT) 17, and providing detailed algorithmic steps (Algorithmic Prompting).29 Step-aware verifiers further refine this by focusing on the correctness of each step.9
   * Search and Planning Strategies: Recognizing the limitations of linear reasoning, researchers have developed methods that explore multiple potential solution paths. Tree-of-Thoughts (ToT) allows models to explore different reasoning branches and self-evaluate progress.10 Graph-of-Thoughts (GoT) proposes representing reasoning as a graph, although current implementations are debated.9 Search algorithms like Monte Carlo Tree Search (MCTS) 6, Best-First Search (BFS) 30, and Beam Search 20 are increasingly used, often guided by reward models or heuristics, to navigate the complex search space of mathematical solutions.
   * Tool Integration: LLMs inherently struggle with precise calculations and complex procedures.29 Tool-augmented approaches address this by enabling LLMs to call external tools. Program-Aided Language Models (PAL) prompt the LLM to generate code (e.g., Python) which is then executed by an interpreter to obtain results, effectively offloading computation.17 More general frameworks like Automatic Reasoning and Tool-use (ART) allow LLMs to dynamically select and use various tools, such as calculators or search engines, within their reasoning process.10
   * Verification and Refinement: To improve reliability, methods involving verification and refinement are common. Verifier models (either trained reward models or rule-based systems) assess the correctness of generated steps or final answers.9 This feedback can be used in search or to prompt the model for self-correction or refinement.6
* Notable Papers/Repos: Significant research is often evaluated on benchmarks like MATH 28 and GSM8K 9, with recent work tackling Olympiad-level problems.26 Key papers introduce techniques like PAL 31, ART 35, ToT 10, STaR 6, and reasoning frameworks like rStar-Math 8 and CoR-Math.42 Relevant repositories include Awesome-LLM-Reasoning 9 and Awesome-System2-Reasoning-LLM.6
The trajectory in mathematical problem-solving clearly moves from simple sequential prompting (CoT) towards more structured and complex reasoning architectures. The adoption of search algorithms (ToT, MCTS) and the integration of external computational tools (PAL, ART) reflect an effort to overcome the inherent limitations of LLMs in multi-step logical deduction and precise calculation, aiming for the more deliberate, System 2-like reasoning characteristic of human experts.3 Furthermore, the focus is shifting beyond merely achieving the correct final answer. The development of process reward models 6 and evaluation methodologies that scrutinize the generated reasoning steps or proofs 5 indicates a growing understanding that flawed reasoning can sometimes lead to correct answers by chance or pattern matching.27 This necessitates evaluating the process of reasoning, not just the outcome, for a true assessment of capability.
3.2 Automated Theorem Proving (ATP)
This specialized area focuses on using LLMs, often in conjunction with Interactive Theorem Provers (ITPs) like Lean 44, Isabelle, or Coq, to generate formal mathematical proofs. The goal is to bridge the gap between informal mathematical language and the rigorous, machine-checkable syntax required by ITPs.
* Key Techniques:
   * LLM-ITP Synergy: The dominant approach involves integrating LLMs with ITPs. LLMs can act as powerful heuristic guides, suggesting promising tactics, retrieving relevant lemmas from large mathematical libraries, or generating proof sketches, while the ITP provides the crucial step-by-step verification needed for formal correctness.17 Lean is a particularly popular ITP in this context.44
   * Expert Iteration and Self-Improvement: Similar to problem-solving, ATP systems benefit from iterative training loops. An LLM generates proofs or conjectures, the ITP attempts to verify them, and the feedback (success, failure, specific errors) is used to refine the LLM, leading to progressively better provers.6
   * LLM-Guided Search: Proof generation can be framed as a search problem within the vast space of possible tactic applications. LLMs are used to guide search algorithms like Best-First Search (BFS) 30 or MCTS 21 by predicting promising steps or evaluating proof states.
   * Formal Proof Data Synthesis: A major bottleneck in training LLM-based provers is the scarcity of large-scale, high-quality formal proof data.47 Techniques like LeanNavigator have been developed to automatically generate millions of new theorems and proofs by exploring the state transition graphs of existing proofs within an ITP.47
   * Auto-Formalization: LLMs are also explored for the task of translating mathematical statements from informal natural language into the formal language required by ITPs.50
* Notable Papers/Repos: Key contributions include systems like BFS-Prover 30 and data generation methods like LeanNavigator.47 Research is often evaluated on benchmarks like MiniF2F.30 Tools like LeanDojo facilitate interaction between LLMs and Lean. Foundational tools include Lean itself 44 and techniques like LeanReasoner.17
The collaboration between LLMs and ITPs is central to progress in LLM-based ATP. While LLMs excel at pattern recognition, heuristic search, and generating plausible text, they inherently lack the mechanisms for guaranteeing logical soundness, often producing proofs with subtle errors.25 Formal ITPs provide this crucial verification layer. Therefore, leveraging the strengths of both—LLMs for suggesting plausible paths and ITPs for ensuring correctness—forms the most viable path forward currently.25 Mirroring trends in general math problem solving, the limited availability of suitable training data (formal proofs) poses a significant challenge. The successful development of automated data synthesis techniques like LeanNavigator 47, which generated a dataset an order of magnitude larger than previous ones and led to improved model performance, highlights that creating high-quality formal training data is a critical research direction within this subfield.
3.3 Symbolic & Neurosymbolic Mathematics
This area investigates the use of LLMs for tasks involving the manipulation of symbolic expressions and explores the integration of neural models with symbolic reasoning methods to enhance reliability, interpretability, and rule-following precision.
* Key Techniques:
   * Symbolic Solvers Integration: LLMs can be prompted to generate steps involving symbolic manipulation or to interact with external symbolic solvers (like SymPy or Mathematica) to perform calculations or simplifications.10 Logic-LM 10 and SatLM 10 are examples integrating logical solvers.
   * Neurosymbolic Representations: A novel approach involves bridging the neural and symbolic realms more deeply. One method encodes the LLM's internal hidden states into structured neurosymbolic vectors using frameworks like Vector Symbolic Algebras (VSAs). Symbolic algorithms can then operate directly on these vector representations within a dedicated neurosymbolic space. The results are decoded back and integrated into the LLM's processing, significantly improving performance on rule-based tasks.53
   * Constrained Decoding: To ensure that LLM outputs conform to the strict syntax required by symbolic systems or programming languages, constrained decoding techniques force the generation process to adhere to a predefined grammar. However, overly strict constraints can sometimes hinder the LLM's ability to perform intermediate reasoning steps. Techniques like CRANE aim to balance syntactic validity with reasoning flexibility by augmenting the grammar to allow for intermediate "thought" tokens.56
   * Symbolic Structure Prediction: LLMs can be trained to predict the underlying symbolic structure of a problem or its solution. For example, predicting the necessary mathematical operators for solving Partial Differential Equations (PDEs) can significantly prune the search space for symbolic regression techniques.57
   * Symbolic Mixture of Experts (Symbolic-MoE): This framework uses symbolic, text-based representations of skills (e.g., "algebra," "calculus") to route specific problem instances to the most suitable pre-trained expert LLM from a pool, enabling instance-level specialization without gradient-based routing.58
* Notable Papers/Repos: Research in this area includes the work on neurosymbolic representations using VSAs 53, constrained decoding with CRANE 56, LLMs for PDE operator prediction 57, Symbolic-MoE 58, and integrations like Logic-LM 10, SatLM 10, and SymbLLM.17
A key motivation for exploring symbolic and neurosymbolic approaches stems from the recognized limitations of purely neural LLMs in tasks demanding high precision, strict rule adherence, and verifiable correctness—hallmarks of mathematical and logical reasoning.29 Neurosymbolic methods, such as encoding hidden states into VSAs 53, offer a potential pathway to combine the pattern-matching strengths of LLMs with the rigor of symbolic computation, leading to reported improvements in accuracy, reliability, and even interpretability.53 However, integrating external symbolic systems or enforcing constraints introduces its own complexities. The finding that overly strict grammatical constraints in decoding can negatively impact reasoning performance 56 highlights a crucial tension: ensuring syntactically correct output for symbolic tools must be balanced with allowing the LLM sufficient "cognitive workspace" to generate intermediate reasoning steps. Techniques like CRANE 56, which modify the grammar to accommodate such steps, represent efforts to find this balance.
3.4 Multimodal Mathematical Reasoning
This rapidly growing area addresses mathematical problems that incorporate non-textual information, such as diagrams in geometry, plots and charts in data analysis, tables, or even handwritten equations.
* Key Techniques:
   * Multimodal LLMs (MLLMs): The core enablers are MLLMs specifically designed to process inputs from multiple modalities, primarily text and images.2 These models typically employ vision encoders alongside language models, with mechanisms to fuse information from both streams.
   * Multimodal Benchmarks: Progress is driven and measured by specialized benchmarks that include visual elements. Examples include MathVista (diverse math tasks with visual contexts) 2, ScienceQA (multidisciplinary science questions, often with images/text) 11, MATH-Vision (competition math problems with visual contexts) 2, GeoQA/GeoEval (geometry problems) 2, FigureQA (scientific figure understanding) 11, and various chart/table QA datasets.11
   * Multimodal Processing Pipelines: Solving multimodal math problems often requires complex pipelines. These might involve Optical Character Recognition (OCR) to extract text from images, diagram parsing to understand geometric relationships, table understanding, and joint reasoning over the extracted visual information and the textual problem description.2 Training strategies may involve two stages (e.g., visual description then reasoning) or visual-centric supervision.2
   * Multimodal Data Synthesis: Given the scarcity of high-quality multimodal math data, significant effort goes into creating it. This includes collecting image-text pairs from existing sources, synthesizing new question-answer pairs for existing images, or using LLMs themselves to generate and validate multimodal datasets.2 MathV360K is a large dataset created through collection and synthesis.11
* Notable Papers/Repos: Key resources include surveys on MLLMs in math 2, papers introducing benchmarks like MathVista 59, ScienceQA 61, and MathV360K 64, and evaluations of MLLMs like GPT-4V, Gemini Vision, and Qwen-VL on these benchmarks.2 The Awesome-Multimodal-LLM-for-Math-STEM repository is a relevant curated list.11
Extending mathematical reasoning to multimodal contexts introduces significant complexity beyond text-only problems. It requires not only logical deduction but also sophisticated visual perception (e.g., interpreting intricate diagrams, recognizing relationships in charts) and the ability to seamlessly integrate information across modalities.2 Current state-of-the-art MLLMs, while showing promise, still exhibit a substantial performance gap compared to human experts, particularly on benchmarks featuring complex, competition-level problems with visual elements like MATH-Vision 60 or the challenging components of MathVista.59 This gap underscores the difficulty of the task.2 As observed in other areas of LLM math research, the availability of large-scale, diverse, and high-quality training data is a critical bottleneck.2 The creation of datasets like MathV360K 64 and the focus on data synthesis within the multimodal math community highlight that data curation and generation are paramount activities needed to drive progress in enabling MLLMs to effectively reason about mathematics in visually rich contexts.
3.5 Training, Fine-tuning, and Optimization Strategies
Developing LLMs capable of strong mathematical reasoning often requires specialized training and optimization techniques beyond standard pre-training and instruction following.
* Key Techniques:
   * Domain-Specific Pre-training: Continuing the pre-training phase of a base LLM on large corpora rich in mathematical content (e.g., scientific papers, textbooks, code, web math content like OpenWebMath) is a common strategy to imbue models with foundational mathematical knowledge and terminology.4
   * Supervised Fine-Tuning (SFT): Models are fine-tuned on datasets consisting of mathematical problems paired with their step-by-step solutions (reasoning paths) and final answers. Instruction tuning, a form of SFT using formatted instructions, is also widely used.4
   * Reinforcement Learning (RL): RL frameworks are increasingly employed to optimize LLMs for reasoning tasks, allowing models to learn through exploration and feedback signals.
      * Algorithms: Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), and specialized variants like Group Relative Policy Optimization (GRPO) are commonly used.6
      * Feedback Mechanisms: Feedback can come from human preferences (RLHF), AI models (RLAIF), or automatically computed rewards. Outcome Reward Models (ORMs) evaluate the final answer's correctness, while Process Reward Models (PRMs) provide more granular feedback by evaluating each intermediate reasoning step.6
   * Self-Improvement / Self-Training: These paradigms involve the LLM generating its own training data, often through exploration (e.g., generating multiple solution attempts using search algorithms like MCTS), evaluating these attempts (using reward models or verification), and then learning from the successful or preferred reasoning paths. Examples include STaR (Self-Taught Reasoner), ReST (Reinforced Self-Training), Quiet-STaR, and rStar-Math.6
   * Data Augmentation and Synthesis: To overcome data limitations, various techniques are used to create more or better training data. This includes rephrasing problems, varying difficulty levels (MetaMath, WizardMath), fusing multiple problems to teach relational concepts (MathFusion) 2, or generating large proof datasets (LeanNavigator).47
   * Knowledge Distillation: Training smaller, more efficient models by using the outputs (reasoning traces) generated by larger, more capable teacher models.8
   * Efficient Fine-tuning: Applying parameter-efficient fine-tuning techniques like LoRA 55 or specialized methods like ReFT (Representation Finetuning) 6 in the context of mathematical reasoning.
* Notable Papers/Repos: Key research includes work on RL techniques like DeepSeek-R1's GRPO 6, reward modeling approaches like Math-Shepherd 20 and OmegaPRM 20, self-improvement methods like STaR 6 and rStar-Math 8, and data synthesis strategies like MetaMath 4 and MathFusion.4 The Awesome-System2-Reasoning-LLM repository 6 tracks many of these developments.
The development of highly capable reasoning models increasingly relies on techniques beyond standard SFT. Reinforcement Learning, particularly when guided by fine-grained Process Reward Models (PRMs), and Self-Improvement loops (which often incorporate RL and search internally) are proving highly effective.6 These methods allow models to explore the solution space, learn from their mistakes, and refine their reasoning processes in a way that supervised learning alone struggles to achieve. This shift towards learning from generated processes is central to the emergence of "Reasoning LLMs" or "System 2" models.3 Nevertheless, the quality and diversity of the data used for any training paradigm remain paramount. The significant focus on data synthesis, augmentation, and particularly the generation of high-quality reasoning steps (not just final answers) underscores that effective data engineering is fundamental to building better mathematical LLMs.2 The process is becoming as important as the product in training data.
4. Prominent LLMs for Mathematics
The landscape of LLMs applied to mathematics includes both general-purpose models evaluated on math tasks and models specifically developed or adapted for mathematical reasoning.
* Specialized Math LLMs: Several models have been released with a specific focus on mathematics, often achieved through continued pre-training on math-related data or fine-tuning on mathematical problem-solving datasets.
   * DeepSeekMath: A family of models (e.g., 7B) initialized from DeepSeek-Coder and further pre-trained on math-heavy web data (sourced via FastText classification on Common Crawl and OpenWebMath) along with code and language data. Base, Instruct, and RL (using GRPO) versions have been released, showing strong performance on benchmarks like MATH.12
   * Qwen-Math / Qwen2.5-Math: Math-focused versions within the Qwen model family, demonstrating strong results on math benchmarks, sometimes through self-improvement techniques.36
   * InternLM-Math: Models (e.g., 7B, 20B) from the InternLM family, specifically adapted for math. They offer base and SFT checkpoints and show competitive performance on GSM8K and MATH.12
   * Minerva: A series of models (8B, 62B, 540B) from Google, fine-tuned on scientific and mathematical text, demonstrating strong few-shot performance on math benchmarks.17
   * Llemma: Open language models (7B, 34B) specifically pre-trained for mathematics, showing good performance on MATH and MiniF2F.51
   * WizardMath: Models fine-tuned using reinforced Evol-Instruct, achieving high scores on math benchmarks.4
   * MetaMath: Focuses on augmenting math problems for fine-tuning, leading to improved reasoning.4
   * Other Math-Specific Models: Goat 17, MathGLM.17
* General LLMs Evaluated on Math: Many leading general-purpose LLMs are routinely evaluated on mathematical reasoning benchmarks as a measure of their overall capability.
   * OpenAI Models: GPT-3, GPT-4, and GPT-4o are frequently used as baselines or SOTA comparisons.3 OpenAI has also released specialized "reasoning models" like o1 and o3-mini, often trained using RL and search, which show exceptionally strong performance on math and coding tasks.3
   * Google Models: PaLM, Flan-PaLM 14, and the Gemini series (including Pro, Ultra, Flash, and experimental versions) are strong contenders, often featuring in benchmark leaderboards.2
   * Anthropic Models: The Claude series (including Opus, Sonnet, Haiku, and newer versions like 3.5/3.7) are also frequently benchmarked and show competitive performance.17
   * Meta Models: The Llama series (Llama 2, Llama 3, Llama 3.3, Llama 4) are widely used open-source base models, often fine-tuned for specific tasks including math.4
   * DeepSeek General/Reasoning Models: Besides DeepSeekMath, the general DeepSeek models (V2, V3) and the reasoning-focused R1 model (trained with RL) are highly capable, particularly in reasoning and coding.3
   * Mistral Models: Mistral 7B, Mixtral, and newer models are popular open-source options evaluated on math tasks.4
   * Other Notable General Models: Command R(+) 12, Grok 12, Phi series 12, etc.
* Multimodal LLMs (MLLMs): Models capable of processing visual input are evaluated on multimodal math benchmarks.
   * Key MLLMs: GPT-4V(ision) 17, Gemini (with vision capabilities) 2, Qwen-VL 12, LLaVA variants (including LLaVA-o1, Math-LLaVA) 2, AtomThink.17
The model landscape reveals a dual approach to improving mathematical reasoning. On one hand, general-purpose flagship models continue to improve their mathematical capabilities through scale and better training data. On the other hand, specialized models, often derived from strong base models (like coding models 66) and subjected to continued pre-training on math/code corpora or intensive fine-tuning/RL on mathematical reasoning data, frequently achieve state-of-the-art results on dedicated math benchmarks.17 This strongly suggests that while general capabilities provide a foundation, domain-specific adaptation significantly enhances performance in complex areas like mathematics. Furthermore, the emergence and high performance of explicitly designated "reasoning models" like OpenAI's 'o' series and DeepSeek's 'R' series, often trained using advanced RL and search techniques 3, point towards a potential divergence in training paradigms. These models appear optimized specifically for complex, multi-step tasks like math and coding, potentially employing different architectures or training objectives compared to standard instruction-tuned chat models, representing a distinct class focused on deep reasoning.3
5. Essential Datasets & Benchmarks
Datasets and benchmarks are the bedrock upon which progress in LLM mathematical reasoning is built and measured. They serve not only as training material but also as crucial tools for evaluating model capabilities, comparing different approaches, and identifying areas needing improvement. The landscape of math benchmarks is evolving, moving beyond simple accuracy on basic problems towards assessing deeper reasoning, robustness, and the validity of the solution process itself.2
* Key Benchmark Categories:
   * General Math Problem Solving (Text-based): This is the most established category.
      * Grade School Level: Datasets like GSM8K (Grade School Math 8K) 9 feature multi-step arithmetic word problems. Variants like GSM-Plus and GSM-Symbolic test robustness and symbolic understanding.2 Other datasets include SVAMP 28, AddSub 17, and ASDiv.40
      * Competition Level: The MATH dataset 17 contains challenging problems from high school math competitions (AMC, AIME). Other benchmarks focus specifically on AIME problems 28, Olympiad-level questions (OlympiadBench 2), or problems from specific national exams like the USAMO 26, Hungarian exams 41, or Chinese Gaokao.77
      * University/Advanced Level: Benchmarks are emerging to test higher-level math. U-MATH 17 includes university-level problems. SciBench features college-level problems from physics, chemistry, and math textbooks, often requiring complex calculations like calculus.13 GPQA (Graduate-Level Google-Proof Q&A) contains extremely difficult expert-level questions in STEM fields.58 The math sections of MMLU 40 and its more challenging successor MMLU-Pro 58 also fall into this category.
   * Formal Theorem Proving: Benchmarks in this area typically consist of formalized theorem statements. MiniF2F is a key benchmark containing formal Olympiad-level problems translated into languages like Lean, Isabelle, and Metamath.17 Other relevant datasets include NaturalProofs 17, ProofNet 52, HolStep, and CoqGym.17
   * Multimodal Mathematics: These benchmarks explicitly include visual elements. MathVista is a diverse benchmark aggregating tasks requiring mathematical reasoning in visual contexts.2 ScienceQA contains multimodal science questions, many involving math, often with diagrams or text contexts.11 MATH-Vision focuses specifically on competition math problems with visual elements.2 Geometry-focused benchmarks include GeoQA 2 and GeoEval.2 Others involve understanding figures (FigureQA 11), charts (ChartQA variants 11), or documents (DocReason25K 11). MM-MATH aims for process evaluation in multimodal math.2
   * Specialized/Diagnostic Benchmarks: Some benchmarks are designed for specific evaluation goals. MathEval provides a comprehensive suite covering many domains and difficulty levels.13 MR-GSM8K tests meta-reasoning by asking models to evaluate solutions and identify errors.39 FOLIO assesses first-order logic reasoning.17 TabMWP requires reasoning over both text and tabular data.11
* Training Datasets: While benchmarks are used for evaluation, models are trained on large datasets, often including step-by-step solutions. The training splits of benchmarks like GSM8K 40 and MATH 37 are widely used. Other notable training sets include MathInstruct 2, large web-crawled math corpora like OpenWebMath 66, datasets focused on process rewards like PRM800K 17, synthesized datasets like MetaMathQA 17, and large multimodal datasets like MathV360K.11
Table 1: Key Benchmarks for LLMs in Mathematics


Benchmark Name
	Source (Example Paper/Link)
	Focus Area
	Key Characteristics
	GSM8K
	Cobbe et al. (2021) 40
	Grade School Math Word Problems
	~8.5K problems, 2-8 step arithmetic reasoning, natural language.
	MATH
	Hendrycks et al. (2021) 37
	Competition Math (HS Level)
	12.5K problems (Algebra, Geometry, PreCalc, Num Theory, C&P), step-by-step solutions.
	MiniF2F
	Zheng et al. (2021) 52
	Formal Theorem Proving
	~488 Olympiad/HS/UG problems formalized in Lean, Isabelle, Metamath.
	MathVista
	Lu et al. (2023) 59
	Multimodal Math Reasoning
	~6K problems from 28+ datasets, diverse visual contexts (charts, diagrams, etc.).
	ScienceQA
	Lu et al. (2022) 62
	Multimodal Science QA (incl. Math)
	~21K multiple-choice questions, image/text contexts, explanations provided.
	MMLU-Pro
	Wang et al. (2024) 79
	Advanced Multitask QA (incl. Math)
	Enhanced MMLU, >12K questions, 10 choices, more reasoning-focused.
	GPQA
	Rein et al. (2023) 73
	Graduate-Level STEM QA
	448 expert-written, "Google-proof" multiple-choice questions (Bio, Phys, Chem).
	SciBench
	Wang et al. (2023) 78
	College-Level Scientific Problems
	Problems from Chem, Phys, Math textbooks, requires domain knowledge, complex calc.
	MATH-Vision
	Wang et al. (2024) 60
	Multimodal Competition Math
	3K+ problems from real competitions with visual contexts, 16 disciplines, 5 levels.
	MR-GSM8K
	Liu et al. (2023) 41
	Meta-Reasoning (Error Detection)
	Based on GSM8K, asks models to score solutions and identify errors.
	The benchmark landscape reflects the field's rapid maturation. There is a clear trend towards creating more challenging benchmarks that test reasoning on harder problems (Olympiad, graduate-level) 26, incorporate multimodality 2, assess robustness against perturbations 2, and evaluate the reasoning process itself rather than just the final answer.2 This evolution is necessary as models quickly saturate performance on simpler benchmarks like GSM8K.41 Furthermore, the issue of data contamination—where benchmarks might inadvertently become part of models' training data, inflating scores—is gaining attention. This motivates the use of newer problems, such as those from recent exams 26, or dynamically updated benchmarks to ensure evaluations reflect genuine generalization ability rather than memorization.41
6. Proposed Repository Structure
Analysis of Existing Structures:
Reviewing existing "awesome" lists within the AI/ML domain on GitHub 9 reveals several common organizational patterns:
1. By Resource Type: Grouping links into categories like "Papers," "Code," "Datasets," "Models," "Blogs," "Surveys." This is simple but can make finding resources for a specific technique or task difficult.
2. By Technique/Methodology: Organizing resources based on the underlying technique, such as "Chain-of-Thought," "Reinforcement Learning," "Quantization," "Tool Use," "Multimodal Fusion." This is useful for researchers focusing on specific methods.6
3. By Task/Application: Structuring the list around specific tasks the resources address, like "Mathematical Problem Solving," "Theorem Proving," "Code Generation," "Agent Learning".11
4. Chronological/Recency: Some lists prioritize recent papers or updates.15
5. Hybrid Approaches: Many lists combine these strategies, often having top-level categories for resource types or broad areas, with sub-categories based on techniques or tasks.
Proposed Structure for awesome-LLM-in-math:
To maximize usability and clarity for a repository focused specifically on LLMs in mathematics, a hierarchical structure that balances task, technique, and resource type is recommended. This allows users with different goals—whether exploring a specific technique, finding resources for a particular math domain, or looking for datasets—to navigate effectively.
Top-Level Categories:
* 📜 Surveys & Overviews:
   * Links to key survey papers covering LLMs in mathematical reasoning, optimization, multimodal math, specific techniques (CoT, RL), and general LLM capabilities.
   * Optionally, links to other highly relevant "awesome" lists (e.g., general reasoning, multimodal).
* 🧠 Core Reasoning Techniques:
   * Organized by fundamental technique categories. Each sub-section should link to seminal papers, key survey sections, and notable open-source implementations.
   * Chain-of-Thought & Prompting Strategies (e.g., Basic CoT, Auto-CoT, Algorithmic Prompting)
   * Search & Planning (e.g., Tree-of-Thoughts, Graph-of-Thoughts, MCTS, BFS, Beam Search)
   * Reinforcement Learning & Reward Modeling (e.g., PPO, DPO, GRPO, ORMs, PRMs, RLHF/RLAIF)
   * Self-Improvement & Self-Training (e.g., STaR, ReST, rStar-Math, Self-Refine)
   * Tool Use & Augmentation (e.g., PAL, ART, Calculator/Solver Integration)
   * Neurosymbolic Methods (e.g., VSA integration, Constrained Decoding)
* 🧮 Mathematical Domains & Tasks:
   * Organized by specific mathematical areas or task types. Each sub-section should link to relevant papers applying techniques to this domain, specialized models, and task-specific datasets/benchmarks.
   * Arithmetic & Word Problems (e.g., GSM8K-related work)
   * Algebra
   * Geometry (potentially linking to Multimodal section)
   * Calculus & Analysis
   * Competition Math (e.g., MATH, AIME, Olympiad)
   * Formal Theorem Proving (e.g., Lean, Isabelle, MiniF2F)
   * Symbolic Manipulation
* 🖼️ Multimodal Mathematical Reasoning:
   * A dedicated section for resources dealing with visual or other non-textual mathematical information.
   * Papers (Techniques, Analyses)
   * Models (MLLMs evaluated on math)
   * Datasets & Benchmarks (e.g., MathVista, ScienceQA, MATH-Vision, GeoQA)
* 🤖 Models:
   * Links to model cards, repositories, or official pages for prominent LLMs relevant to mathematics.
   * Math-Specialized LLMs (e.g., DeepSeekMath, QwenMath, InternLM-Math, Llemma)
   * Reasoning-Focused LLMs (e.g., OpenAI o1/o3, DeepSeek R1)
   * Leading General LLMs (Frequently benchmarked models like GPT-4, Gemini, Claude)
* 📊 Datasets & Benchmarks:
   * Links to datasets for training and evaluation, possibly categorized further.
   * Problem Solving Benchmarks (e.g., GSM8K, MATH, SciBench, GPQA, MMLU-Pro)
   * Theorem Proving Benchmarks (e.g., MiniF2F)
   * Multimodal Benchmarks (e.g., MathVista, ScienceQA, MATH-Vision)
   * Training Datasets (e.g., MathInstruct, OpenWebMath, MathV360K)
* 🛠️ Tools & Libraries:
   * Links to software relevant for working with LLMs in mathematics.
   * Interactive Theorem Provers (e.g., Lean 46)
   * LLM Interaction Libraries (e.g., LangChain, LMDeploy 69, Guidance 33)
   * Code Execution Backends (for PAL-like approaches)
   * Evaluation Frameworks (e.g., OpenCompass 68)
   * Data Processing Tools 13
Rationale: This structure provides multiple entry points for users. Someone interested in "Tree-of-Thoughts" can go to Core Reasoning Techniques -> Search & Planning. Someone working on geometry problems can check Mathematical Domains & Tasks -> Geometry and Multimodal Mathematical Reasoning. Someone looking for the latest benchmarks can go directly to Datasets & Benchmarks. Separating core techniques from domain applications helps clarify foundational methods versus their use cases.
Formatting Conventions: Clear Markdown headings (##, ###) should be used for sections and sub-sections. Each entry should include a concise description and a clearly formatted link. A consistent format for citations is recommended, for example:
-(Link_to_Paper_PDF_or_arXiv) - Author 1, Author 2, et al. (*Conference/Journal/Year*). Short description of key contribution.
-(Link_to_GitHub_or_Website) - Brief description of the tool or code.
- [Dataset Name](Link_to_Dataset_Page) - Focus, scale, source paper.
Emphasis should be placed on providing direct links to the primary source (e.g., arXiv page for papers, official GitHub repository for code, dataset homepage or Hugging Face page).
7. Curated Resource List (Preview)
This section provides a preview of the curated lists of papers, repositories, and datasets, organized according to the proposed structure. The final repository would contain fully populated versions of these lists.
Table 2: Curated Papers (Representative Examples)
Title
	Authors
	Venue/Year
	Link
	Key Contribution/Area
	A Survey on Mathematical Reasoning and Optimization with Large Language Models
	Forootani, A.
	arXiv:2503.17726
	Link
	Survey (Math Reasoning, Optimization, CoT, Tools, RL)
	A Survey on Feedback-based Multi-step Reasoning for LLMs on Mathematics
	Wei, T.-R., et al.
	arXiv:2502.14333
	Link
	Survey (Feedback Mechanisms, PRM/ORM, Math Reasoning)
	Reasoning LLMs: The Path Towards Advanced Cognitive Systems
	Zeng, Z., et al.
	arXiv:2502.17419
	Link
	Survey (System 2 Reasoning, MCTS, RL, Self-Improvement)
	A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model
	Yan, Y., et al.
	arXiv:2412.11936
	Link
	Survey (Multimodal Math Reasoning, Benchmarks, Methods)
	Tree of Thoughts: Deliberate Problem Solving with Large Language Models
	Yao, S., et al.
	NeurIPS 2023
	Link
	Technique (Tree Search, Planning)
	PAL: Program-Aided Language Models
	Gao, L., et al.
	ICML 2023
	Link
	Technique (Tool Use, Code Generation)
	ART: Automatic multi-step reasoning and tool-use for large language models
	Paranjape, B., et al.
	arXiv:2303.09014
	Link
	Technique (Tool Use, Automatic Prompting)
	STaR: Bootstrapping Reasoning With Reasoning
	Zelikman, E., et al.
	arXiv:2203.14465
	Link
	Technique (Self-Improvement, SFT)
	Small LLMs Can Master Reasoning with Self-Evolved Deep Thinking (rStar-Math)
	Qi, Z., et al.
	arXiv:2502.10000
	Link
	Technique (Self-Improvement, MCTS, RL, Math Reasoning)
	Deepseekmath: Pushing the limits of mathematical reasoning in open language models
	Shao, Z., et al.
	arXiv:2402.03300
	Link
	Model (DeepSeekMath, Training Pipeline)
	LeanNavigator: Generating Diverse Theorems and Proofs by Exploring State Graphs
	Jiang, A., et al.
	arXiv:2503.04772
	Link
	Technique (Data Synthesis, Theorem Proving, Lean)
	Improving Rule-based Reasoning in LLMs via Neurosymbolic Representations
	Dhanraj, V., & Eliasmith, C.
	arXiv:2502.01657
	Link
	Technique (Neurosymbolic, VSA, Rule Following)
	MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts
	Lu, P., et al.
	NeurIPS 2023
	Link
	Benchmark (Multimodal Math Reasoning)
	Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad
	Liu, Z., et al.
	arXiv:2503.21934
	Link
	Evaluation (Olympiad Math, Reasoning Process)
	Table 3: Curated Repositories/Code (Representative Examples)
Repository Name
	Link
	Description
	Associated Paper (if any)
	Awesome-LLM-Reasoning (atfortes)
	Link
	Curated list of papers/resources on LLM reasoning (CoT, ToT, etc.)
	-
	Awesome-System2-Reasoning-LLM
	Link
	Curated list focusing on System 2 reasoning (RL, MCTS, Self-Improve)
	Survey: arXiv:2502.17419
	Awesome-Multimodal-LLM-for-Math-STEM
	Link
	Curated list for MLLMs in Math/STEM, focusing on datasets.
	-
	pal (reasoning-machines)
	Link
	Implementation of Program-Aided Language Models (PAL).
	PaL: Program-Aided Language Models (arXiv:2211.10435)
	guidance (microsoft)
	Link
	Library for controlling LLMs (used in ART paper).
	ART: Automatic multi-step reasoning... (arXiv:2303.09014)
	DeepSeek-Math
	Link
	Code and model checkpoints for DeepSeekMath models.
	Deepseekmath: Pushing the limits... (arXiv:2402.03300)
	InternLM (InternLM)
	Link
	Repository for InternLM models, including InternLM-Math.
	InternLM Technical Report
	Lean 4 (leanprover)
	Link
	Source code for the Lean 4 programming language & theorem prover.
	-
	Tree-of-Thoughts-LLM (kyegomez)
	Link
	Implementation of Tree of Thoughts.
	Tree of Thoughts: Deliberate Problem Solving... (arXiv:2305.10601)
	Table 4: Curated Datasets (Representative Examples)


Dataset Name
	Link
	Description
	Source Paper
	MATH
	HF Link
	12.5K challenging competition math problems (HS level) with solutions.
	Measuring Mathematical Problem Solving... (arXiv:2103.03874) 37
	GSM8K
	HF Link
	8.5K grade school math word problems requiring 2-8 arithmetic steps.
	Training Verifiers to Solve Math Word Problems (arXiv:2110.14168) 40
	MiniF2F
	GitHub Link
	~488 formal Olympiad/HS/UG math problems (Lean, Isabelle, Metamath).
	MiniF2F: a cross-system benchmark... (arXiv:2109.00110) 52
	MathVista
	Project Page
	~6K multimodal math problems aggregated from 28+ datasets.
	MathVista: Evaluating Mathematical Reasoning... (arXiv:2310.02255) 59
	ScienceQA
	Project Page
	~21K multimodal multiple-choice science questions (incl. math) w/ explanations.
	Learn to Explain: Multimodal Reasoning... (arXiv:2209.09958) 62
	MMLU-Pro
	HF Link
	>12K challenging multitask QA questions (enhanced MMLU) with 10 choices.
	MMLU-Pro: A More Robust and Challenging... (arXiv:2406.01574) 79
	GPQA
	GitHub Link
	448 expert-written, graduate-level "Google-proof" STEM questions.
	GPQA: A Graduate-Level Google-Proof Q&A Benchmark (arXiv:2311.12022) 73
	SciBench
	Project Page
	College-level Chemistry, Physics, Math problems from textbooks.
	SciBench: Evaluating College-Level... (arXiv:2307.10635) 78
	MathV360K
	HF Link
	40K images + 360K QA pairs for multimodal math reasoning (collected/synth).
	Math-LLaVA: Bootstrapping Mathematical Reasoning... (arXiv:2406.13017) 64
	OpenWebMath
	(Used internally by DeepSeek)
	Large corpus of math-related web text used for pre-training.
	Deepseekmath: Pushing the limits... (arXiv:2402.03300) 66
	8. Challenges & Future Directions
Despite the remarkable progress in applying LLMs to mathematical reasoning, significant challenges remain, and numerous avenues for future research are actively being pursued.
Recap of Challenges:
Throughout this analysis, several recurring challenges have emerged:
* Reliability and Soundness: LLMs frequently make errors in calculation (numerical precision issues) and logical deduction (logical consistency). They can "hallucinate" plausible but incorrect reasoning steps, which is particularly problematic in formal proof generation where a single error invalidates the entire proof.1
* Complexity Handling: While performance on simpler problems has improved, current models still struggle with very complex, multi-step problems, long proofs, or problems requiring deep conceptual understanding beyond pattern matching.17
* Multimodal Integration: Effectively processing and reasoning over combined textual and visual information (diagrams, charts, equations within images) remains a significant hurdle.2
* Evaluation: Designing benchmarks and metrics that accurately assess true reasoning capabilities, measure robustness, evaluate the process (not just the outcome), and avoid data contamination is an ongoing challenge.1
* Data Scarcity and Quality: High-quality, large-scale datasets, especially those with detailed reasoning steps, formal proofs, or diverse multimodal contexts, are often lacking, hindering training and evaluation.1
* Interpretability and Trust: Understanding how LLMs arrive at their mathematical conclusions and ensuring their outputs are trustworthy remain open questions.1
* Sensitivity and Efficiency: LLM performance can be sensitive to prompt variations, and advanced reasoning techniques involving search or multiple model calls often incur significant computational overhead.1
Emerging Trends & Future Directions:
The research landscape points towards several promising future directions aimed at addressing these challenges:
* Hybrid Neural-Symbolic Approaches: Continued exploration of methods that combine the strengths of LLMs (pattern recognition, heuristics) with symbolic systems (rigor, precision, interpretability) holds significant potential for improving reliability.1
* Robust Verification and Correction: Developing better automated methods for verifying LLM-generated reasoning steps and proofs (beyond reliance on ITPs) and enabling more effective self-correction mechanisms is crucial.1
* Enhanced Tool Integration: Improving the seamless integration of LLMs with a wider range of sophisticated external tools, including symbolic solvers, numerical libraries, and planning systems, will likely boost capabilities.1
* Advanced Training Paradigms: Refining RL techniques (especially with process-based rewards) and self-improvement frameworks appears key to unlocking deeper reasoning abilities.1
* Focus on Reasoning Process: Shifting focus in both training and evaluation towards the intermediate reasoning steps, not just the final answer, to foster more robust and understandable models.2
* Multimodal Advancements: Developing better architectures and training strategies for fusing visual and textual information, along with creating richer multimodal datasets, is essential for progress in this area.1
* Interpretability and Explainability: Research into making the reasoning processes of LLMs more transparent will be vital for building trust and enabling effective debugging.1
* Improved Evaluation: Continued innovation in benchmark design is needed to create evaluations that are challenging, robust, process-oriented, and resistant to contamination.1
* Interactive Reasoning: Exploring frameworks where LLMs collaborate with humans interactively to solve complex mathematical problems.1
Concluding Remarks:
Large Language Models are rapidly becoming more capable mathematical reasoners, transitioning from basic pattern matching to employing complex, multi-step strategies involving search, tool use, and self-correction. This progress is driven by innovations in training methodologies, particularly reinforcement learning and self-improvement, and fueled by increasingly challenging benchmarks and specialized datasets. However, fundamental challenges related to reliability, verification, deep understanding versus heuristics, and robust evaluation persist. The field is characterized by a dynamic interplay between pushing performance boundaries and striving for greater trustworthiness and rigor. Hybrid approaches, sophisticated training paradigms focused on the reasoning process, and continued efforts in data curation and evaluation design appear crucial for future advancements. Curated resources, such as the proposed "awesome-LLM-in-math" repository, play a vital role in helping researchers and practitioners navigate this complex and rapidly evolving landscape, fostering collaboration and accelerating progress towards AI systems with genuine mathematical intelligence.
Works cited
1. A Survey on Mathematical Reasoning and Optimization with Large Language Models - arXiv, accessed April 9, 2025, https://arxiv.org/abs/2503.17726
2. arxiv.org, accessed April 9, 2025, https://arxiv.org/abs/2412.11936
3. From System 1 to System 2: A Survey of Reasoning Large Language Models - arXiv, accessed April 9, 2025, https://arxiv.org/pdf/2502.17419?
4. MathFusion: Enhancing Mathematic Problem-solving of LLM through Instruction Fusion - arXiv, accessed April 9, 2025, https://arxiv.org/html/2503.16212v1
5. Evaluating language models for mathematics through interactions - PMC, accessed April 9, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC11181017/
6. zzli2022/Awesome-System2-Reasoning-LLM: Latest ... - GitHub, accessed April 9, 2025, https://github.com/zzli2022/Awesome-System2-Reasoning-LLM
7. From System 1 to System 2: A Survey of Reasoning Large Language Models - arXiv, accessed April 9, 2025, https://arxiv.org/html/2502.17419v2
8. rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking, accessed April 9, 2025, https://thenaai.org/uploads/admin/article_thumb/20250210/94b8994058899cefa8eeddddb5865651.pdf
9. luban-agi/Awesome-LLM-reasoning - GitHub, accessed April 9, 2025, https://github.com/luban-agi/Awesome-LLM-reasoning
10. atfortes/Awesome-LLM-Reasoning: Reasoning in LLMs ... - GitHub, accessed April 9, 2025, https://github.com/atfortes/Awesome-LLM-Reasoning
11. InfiMM/Awesome-Multimodal-LLM-for-Math-STEM - GitHub, accessed April 9, 2025, https://github.com/InfiMM/Awesome-Multimodal-LLM-for-Math-STEM
12. Awesome-LLM: a curated list of Large Language Model - GitHub, accessed April 9, 2025, https://github.com/turna1/Awesome-Multmodal_LLM
13. Awesome-LLM: a curated list of Large Language Model - GitHub, accessed April 9, 2025, https://github.com/Hannibal046/Awesome-LLM
14. jd-coderepos/awesome-llms: 🤓 A collection of AWESOME structured summaries of Large Language Models (LLMs) - GitHub, accessed April 9, 2025, https://github.com/jd-coderepos/awesome-llms
15. horseee/Awesome-Efficient-LLM: A curated list for Efficient Large Language Models - GitHub, accessed April 9, 2025, https://github.com/horseee/Awesome-Efficient-LLM
16. JShollaj/awesome-llm-interpretability - GitHub, accessed April 9, 2025, https://github.com/JShollaj/awesome-llm-interpretability
17. arxiv.org, accessed April 9, 2025, https://arxiv.org/pdf/2503.17726
18. A Survey on Feedback-based Multi-step Reasoning for Large Language Models on Mathematics - arXiv, accessed April 9, 2025, https://arxiv.org/html/2502.14333v1
19. [2502.14333] A Survey on Feedback-based Multi-step Reasoning for Large Language Models on Mathematics - arXiv, accessed April 9, 2025, https://arxiv.org/abs/2502.14333
20. arxiv.org, accessed April 9, 2025, https://arxiv.org/pdf/2502.14333
21. arxiv.org, accessed April 9, 2025, https://arxiv.org/pdf/2502.17419
22. [2402.06196] Large Language Models: A Survey - arXiv, accessed April 9, 2025, https://arxiv.org/abs/2402.06196
23. Evaluating Large Language Models: A Comprehensive Survey - arXiv, accessed April 9, 2025, https://arxiv.org/pdf/2310.19736
24. Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models - arXiv, accessed April 9, 2025, https://arxiv.org/html/2501.09686v1
25. LeanProgress: Guiding Search for Neural Theorem Proving via Proof Progress Prediction, accessed April 9, 2025, https://arxiv.org/html/2502.17925v2
26. [2503.21934] Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad - arXiv, accessed April 9, 2025, https://arxiv.org/abs/2503.21934
27. [2504.01995] Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics - arXiv, accessed April 9, 2025, https://arxiv.org/abs/2504.01995
28. Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations - arXiv, accessed April 9, 2025, https://arxiv.org/pdf/2502.06453
29. Teaching language models to reason algorithmically - Google Research, accessed April 9, 2025, https://research.google/blog/teaching-language-models-to-reason-algorithmically/
30. [2502.03438] BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving - arXiv, accessed April 9, 2025, https://arxiv.org/abs/2502.03438
31. reasoning-machines/pal: PaL: Program-Aided Language Models (ICML 2023) - GitHub, accessed April 9, 2025, https://github.com/reasoning-machines/pal
32. Shuyib/PAL: Using an LLM to answer math word problems common in IRL. Program aided language modeling. - GitHub, accessed April 9, 2025, https://github.com/Shuyib/PAL
33. ART: Automatic multi-step reasoning and tool-use for large language models, accessed April 9, 2025, https://paperswithcode.com/paper/art-automatic-multi-step-reasoning-and-tool
34. Automatic Reasoning and Tool-use (ART) - Prompt Engineering Guide, accessed April 9, 2025, https://www.promptingguide.ai/techniques/art
35. ART: Automatic multi-step reasoning and tool-use for large language models - arXiv, accessed April 9, 2025, https://arxiv.org/abs/2303.09014
36. hijkzzz/Awesome-LLM-Strawberry: A collection of LLM papers, blogs, and projects, with a focus on OpenAI o1 🍓 and reasoning techniques. - GitHub, accessed April 9, 2025, https://github.com/hijkzzz/Awesome-LLM-Strawberry
37. MATH Dataset - Papers With Code, accessed April 9, 2025, https://paperswithcode.com/dataset/math
38. nlile/hendrycks-MATH-benchmark · Datasets at Hugging Face, accessed April 9, 2025, https://huggingface.co/datasets/nlile/hendrycks-MATH-benchmark
39. MR-GSM8K: A META-REASONING BENCHMARK - OpenReview, accessed April 9, 2025, https://openreview.net/pdf/d5d6c38a884aa9905c80b5013c92718069df6130.pdf
40. GSM8K Dataset - Papers With Code, accessed April 9, 2025, https://paperswithcode.com/dataset/gsm8k
41. MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation - arXiv, accessed April 9, 2025, https://arxiv.org/html/2312.17080v2
42. Chain-of-Reasoning: unified framework for Mathematical Reasoning in LLMs via a Multi-Paradigm Perspective | by SACHIN KUMAR | Medium, accessed April 9, 2025, https://medium.com/@techsachin/chain-of-reasoning-unified-framework-for-mathematical-reasoning-in-llms-via-a-multi-paradigm-2d2255d4c78e
43. [D] How does LLM solves new math problems? : r/MachineLearning - Reddit, accessed April 9, 2025, https://www.reddit.com/r/MachineLearning/comments/1ihsftt/d_how_does_llm_solves_new_math_problems/
44. Lean (proof assistant) - Wikipedia, accessed April 9, 2025, https://en.wikipedia.org/wiki/Lean_(proof_assistant)
45. Lean - Microsoft Research, accessed April 9, 2025, https://www.microsoft.com/en-us/research/project/lean/
46. Programming Language and Theorem Prover — Lean, accessed April 9, 2025, https://lean-lang.org/
47. Generating Millions Of Lean Theorems With Proofs By Exploring State Transition Graphs, accessed April 9, 2025, https://arxiv.org/html/2503.04772v1
48. Beyond Limited Data: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving - arXiv, accessed April 9, 2025, https://arxiv.org/html/2502.00212v1
49. arxiv.org, accessed April 9, 2025, https://arxiv.org/pdf/2503.04772
50. Evaluating Mathematical Reasoning in LLMs - Toloka, accessed April 9, 2025, https://toloka.ai/events/advancing-mathematical-reasoning-in-llms
51. miniF2F-test Benchmark (Automated Theorem Proving) - Papers With Code, accessed April 9, 2025, https://paperswithcode.com/sota/automated-theorem-proving-on-minif2f-test
52. MiniF2F Dataset - Papers With Code, accessed April 9, 2025, https://paperswithcode.com/dataset/minif2f
53. Improving Rule-based Reasoning in LLMs via Neurosymbolic Representations - arXiv, accessed April 9, 2025, https://arxiv.org/html/2502.01657v1
54. Improving Rule-based Reasoning in LLMs via Neurosymbolic Representations - arXiv, accessed April 9, 2025, https://www.arxiv.org/abs/2502.01657
55. arxiv.org, accessed April 9, 2025, https://arxiv.org/pdf/2502.01657
56. CRANE: Reasoning with constrained LLM generation - arXiv, accessed April 9, 2025, https://arxiv.org/html/2502.09061v2
57. From Equations to Insights: Unraveling Symbolic Structures in PDEs with LLMs - arXiv, accessed April 9, 2025, https://arxiv.org/html/2503.09986v1
58. Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning, accessed April 9, 2025, https://arxiv.org/html/2503.05641
59. MathVista: Evaluating Math Reasoning in Visual Contexts, accessed April 9, 2025, https://mathvista.github.io/
60. Measuring Multimodal Mathematical Reasoning with the MATH-Vision Dataset, accessed April 9, 2025, https://proceedings.neurips.cc/paper_files/paper/2024/file/ad0edc7d5fa1a783f063646968b7315b-Paper-Datasets_and_Benchmarks_Track.pdf
61. Daily Papers - Hugging Face, accessed April 9, 2025, https://huggingface.co/papers?q=ScienceQA
62. ScienceQA Dataset - Papers With Code, accessed April 9, 2025, https://paperswithcode.com/dataset/scienceqa
63. SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models - GitHub, accessed April 9, 2025, https://raw.githubusercontent.com/mlresearch/v235/main/assets/wang24z/wang24z.pdf
64. README.md · Zhiqiang007/MathV360K at main - Hugging Face, accessed April 9, 2025, https://huggingface.co/datasets/Zhiqiang007/MathV360K/blame/main/README.md
65. Mathematical Reasoning | Papers With Code, accessed April 9, 2025, https://paperswithcode.com/task/mathematical-reasoning?page=18&q=
66. deepseek-ai/DeepSeek-Math: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models - GitHub, accessed April 9, 2025, https://github.com/deepseek-ai/DeepSeek-Math
67. Updated model card for Qwen2 by Aravind-11 · Pull Request #37192 - GitHub, accessed April 9, 2025, https://github.com/huggingface/transformers/pull/37192
68. InternLM/model_cards/internlm2.5_20b.md at main - GitHub, accessed April 9, 2025, https://github.com/InternLM/InternLM/blob/main/model_cards/internlm2.5_20b.md
69. internlm/internlm2-math-base-7b - Hugging Face, accessed April 9, 2025, https://huggingface.co/internlm/internlm2-math-base-7b
70. Large Language Models and Mathematical Reasoning Failures - arXiv, accessed April 9, 2025, https://arxiv.org/html/2502.11574v1
71. MMLU Pro Benchmark - Vals AI, accessed April 9, 2025, https://www.vals.ai/benchmarks/mmlu_pro-04-04-2025
72. GPQA Benchmark - Vals AI, accessed April 9, 2025, https://www.vals.ai/benchmarks/gpqa-04-04-2025
73. GPQA: A Graduate-Level Google-Proof Q&A Benchmark - Klu.ai, accessed April 9, 2025, https://klu.ai/glossary/gpqa-eval
74. GPQA benchmark 2025 - Metaculus, accessed April 9, 2025, https://www.metaculus.com/questions/21920/gpqa-benchmark-2025/
75. huangting4201/HT-InternLM - GitHub, accessed April 9, 2025, https://github.com/huangting4201/HT-InternLM
76. MMLU-Pro Leaderboard - a Hugging Face Space by TIGER-Lab, accessed April 9, 2025, https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro
77. MathEval: A Comprehensive Benchmark for Evaluating Large Language Models on Mathematical Reasoning Capabilities | OpenReview, accessed April 9, 2025, https://openreview.net/forum?id=DexGnh0EcB
78. SciBench: Evaluating Math Reasoning in Visual Contexts, accessed April 9, 2025, https://scibench-ucla.github.io/
79. MMLU-Pro Dataset | Papers With Code, accessed April 9, 2025, https://paperswithcode.com/dataset/mmlu-pro
80. MathEval Dataset - Papers With Code, accessed April 9, 2025, https://paperswithcode.com/dataset/matheval